---
title: "Lab4"
author: "Marcos Bernier"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: yes
    toc_float: yes
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Tasks

## Task1

```{r}
getwd()
```

## Task2
```{r}
ss <- read.csv("SPRUCE.csv")
tail(ss)
```
## Task3
```{r}
library(s20x)
trendscatter(ss$Height~ss$BHDiameter,f=0.5)

spruce.lm = lm(Height~BHDiameter,data = ss)
height.res = residuals(spruce.lm)
height.fit = fitted(spruce.lm)

plot(height.res~height.fit)
trendscatter(height.res~height.fit)
```

The plot displays a parabolic trend, compared to a more linear (flatlining later) trend in the first plot.

```{r}
plot(spruce.lm, which =1 )
normcheck(spruce.lm, shapiro.wilk = TRUE)
```

The p-value is .29 > 0.05 which means the null hypothesis is accepted and the error is distributed normally

```{r}
round(mean(height.res,4))
```

We shouldn't use a straighjt line model to describe this data because the actual trend is more quadratic, so we need to figure a better line to fit the data more descriptively.

## Task 4

```{r}
quad.lm = lm(Height~BHDiameter + I(BHDiameter ^ 2), data = ss)
summary(quad.lm)
```
```{r}
coef(quad.lm)

plot(ss)
plottttt = function(x)
{
  quad.lm$coef[1] + quad.lm$coef[2] * x + quad.lm$coef[3] * x^2
}

curve (plottttt, lwd = 2, col = "red", add = TRUE)
```
```{r}
quad.fit = fitted(quad.lm)
plot(quad.lm, which = 1)
```

```{r}
normcheck(quad.lm, shapiro.wilk = TRUE)
```

P value is .684. The null hypothesis accepts this.
The model fits better than the previous linear model.

## Task5

```{r}
summary(quad.lm)
```

B0hat, B1hat, B2hat = 0.860896, 1.469592, -0.027457

```{r}
ciReg(quad.lm)
```

Y = 0.860896 + 1.46959x - 0.027457x^2

```{r}
predict(quad.lm, data.frame(BHDiameter = c(15,18,20)))
```
```{r}
predict(spruce.lm, data.frame(BHDiameter = c(15,18,20)))
```

The quad.lm predictions are larger than spruce.lm and correctly allign with quadratic growth.

```{r}
summary(quad.lm)$r.squared
```

R^2 = 0.7741266

```{r}
summary(spruce.lm)$r.squared
```

R^2 = 0.6569146

```{r}
summary(quad.lm)$adj.r.squared
summary(spruce.lm)$adj.r.squared

```

Adjusted r squared gets higher as a model improves with mopre variables. Quad.lm is higher so it is a better fit.

Multiple r squared does the same thing but it is not dependent on the amount/addition of new variables/data to the models.

Quad.lm best explains the variability in height, The Rsquared and AdjRsqaured values are greatest

```{r}
anova(spruce.lm)
```

```{r}
anova(quad.lm)
```

```{r}
anova(spruce.lm, quad.lm)
```

Quad.lm better fits the data as a model.


```{r}
height.qfit = fitted(quad.lm)

TSS = with(ss, sum((Height - mean(Height))^2))
MSS = with(ss, sum((height.qfit - mean(Height))^2))
RSS = with(ss, sum((Height - height.qfit)^2))


TSS
MSS
RSS
```

TSS, MSS, RSS in respective order above.
MSS/TSS below.
```{r}
MSS/TSS
```

## Task 6

```{r}
cooks20x(quad.lm)
```

Cooks distance is used to find outliers in variables. The distance is used to identify data points that might negatively affect the model of regression. So if the distance is great, the impact on the model is great.
The 24th observation number is the most influential in the data for quad.lm.


```{r}
quad2.lm = lm(Height~BHDiameter + I(BHDiameter^2), data = ss[-24,])
summary(quad2.lm)
```


The residual values (min, max, median) are all lower for quad2. Inversely, the multiple Rsquared and adjusted Rsquared values are bigger than quad.lm.
Removing the 24th point changed the model significantly, indicating that Cooks Distance was correct in signifying that the 24th point was influential.

## Task 7

There are two lines with $x_k$ in common :

$L_1: y = \beta_0 + \beta_1x$ AND

$L_2: y = \beta_0 + \delta + (\beta_1 + \beta_2)x$

We will plug in the point $x_k$ and make the equations set equal to eachother because the share the common point:

$y_k = \beta_0 + \beta_1x_k =\beta_0 + \delta + (\beta_1 + \beta_2)x_k$

$x_k$ can be distributed to the RHS: 

$\quad \quad \beta_0 + \beta_1x_k - \beta_0 + \delta + \beta_1x_k + \beta_2x_k$

$\beta_0$ and $\beta_1x$ cancel: 

$\quad \quad 0 = \delta + \beta_2 x_k$

$\quad \quad \delta = -\beta_2 x_k$

$L_2 : y = \beta_0 + \delta + (\beta_1 + \beta_2)x$

Substitute the found value for $\delta$:

$L_2 : y = \beta_0 -\beta_2 x_k + (\beta_1 + \beta_2)x$

$\quad\quad y = \beta_0 + \beta_1x + \beta_2(x-x_k)$

This new equations is $L_1$ with $\beta_2(x-x_k)$ appended

 $I()$ is 1 when $x > x_k$ and 0 for all other conditions.

Leaving : $y = \beta_0 + \beta_1x + \beta_2(x-x_k)I(x>x_k)$



```{r}
ss2 = within(ss, X <- (BHDiameter-18) * (BHDiameter>18))

plot(ss)
coefadder = function(x, coef)
{
  coef[1] + coef[2] * (x) + coef[3] * (x-18) *(x-18 >0)
}
temp = lm(Height~BHDiameter + X, data = ss2)
temp = summary(temp)
curve(coefadder(x,coef=temp$coefficients[,"Estimate"]), add = TRUE, lwd = 2, col = "red")
text(18,16,paste("RSQUARED = " ,round(temp$r.squared,4)))
abline(v=18)
```


## Task 8

```{r}
library(MATH4753BERN0021)
print2()
scatterhist(ss$Height,ss$BHDiameter,"dog","cat")
```

I imported two functions here, a function I wrote called print2, and the scatterhist function from lab1. The print2 function simply print 2 to console output. The scatterhist function generates a scatterplot and a histogram appended to the axes accoridng to a set of x and y values! I used the data included in lab4 to run this function. print2 takes no arguments. It just prints 2 :D
